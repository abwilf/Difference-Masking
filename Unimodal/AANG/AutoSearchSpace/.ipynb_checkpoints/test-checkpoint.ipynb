{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5f06fb-5a8d-4875-96ab-22c8ffdd37e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, RobertaModel, RobertaTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# sent = 'In phosphotriesterase and physostigmine-treated mice, a 4- and 2-fold higher << sarin >> dose, respectively, was needed to cause a 50% inhibition of brain [[ AChE ]] activity.'\n",
    "sent = 'In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.'\n",
    "seed_word = 'chemistry'\n",
    "input_chem = tokenizer(seed_word, return_tensors=\"pt\")\n",
    "output_chem = model(**input_chem)\n",
    "\n",
    "input_sent = tokenizer(sent, return_tensors=\"pt\")\n",
    "output_sent = model(**input_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d3032dbd-23f3-4d84-b74c-1c974b7485a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_chem.last_hidden_state[0][1]\n",
    "encoded_sent = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "output_ = output_sent.last_hidden_state.squeeze()\n",
    "\n",
    "sim = []\n",
    "word_sim = {}\n",
    "for idx, word in enumerate(encoded_sent['input_ids']):\n",
    "    token_ids_word = np.where(np.array(encoded_sent.word_ids()) == idx)\n",
    "    word_tokens_output = output_[token_ids_word]\n",
    "    # word_tokens_output = word_tokens_output.mean(dim=0)\n",
    "    sim.append(torch.cosine_similarity(output_chem.last_hidden_state.squeeze()[1:-1], word_tokens_output).detach().numpy()[0])\n",
    "    # sim.append(cosine_similarity([output_chem.last_hidden_state[0][1].detach().numpy()], [word_tokens_output.detach().numpy()])[0][0])\n",
    "    word_sim[word] = sim[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "44336aec-a036-4c86-af91-92ce5ddfa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = np.array(sim)\n",
    "sim = np.arccos(sim) / np.pi\n",
    "sim /= sim.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "17176f31-5bc5-4b78-ba97-8464196a31a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1999,  4164,  2007,  2122,  3463,  1010,  2057, 11548,  2008,\n",
       "          1996,  3595,  3258,  1997,  4013,  1011, 20187,  1026,  1026, 22330,\n",
       "         18715,  3170,  1028,  1028,  1998,  1050,  2546,  1011,  1164,  2497,\n",
       "         13791, 10572,  2011,  1031,  1031, 22975, 14141,  1033,  1033,  2064,\n",
       "          2022, 19872,  2011,  6678,  1997,  1031,  6187,  1006,  1016,  1009,\n",
       "          1007,  1033,  1045,  1999,  5292,  8197, 12702, 20011,  2140,  4442,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sent['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4f410-bd6c-48b0-ab45-4b5091867b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(len(arr)), p=arr, size=int(mask_ratio*len(arr)), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "eaddc65a-399c-4956-87bf-8006b612ef7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In -> chemistry:  0.73156077\n",
      "concert -> chemistry:  0.87898844\n",
      "with -> chemistry:  0.64207655\n",
      "these -> chemistry:  0.59284836\n",
      "results, -> chemistry:  0.9777651\n",
      "we -> chemistry:  0.89152473\n",
      "highlighted -> chemistry:  0.9565176\n",
      "that -> chemistry:  0.96629876\n",
      "the -> chemistry:  0.60394984\n",
      "secretion -> chemistry:  0.83476865\n",
      "of -> chemistry:  0.7530356\n",
      "pro-inflammatory -> chemistry:  0.96491736\n",
      "<< -> chemistry:  0.91706\n",
      "cytokine -> chemistry:  0.9815494\n",
      ">> -> chemistry:  0.7909668\n",
      "and -> chemistry:  0.88205796\n",
      "NF-κB -> chemistry:  0.8645779\n",
      "activation -> chemistry:  0.85931265\n",
      "induced -> chemistry:  0.81914693\n",
      "by -> chemistry:  0.7653643\n",
      "[[ -> chemistry:  0.7459818\n",
      "TCDD -> chemistry:  0.96978515\n",
      "]] -> chemistry:  0.84205204\n",
      "can -> chemistry:  0.79442346\n",
      "be -> chemistry:  0.8243005\n",
      "mediated -> chemistry:  0.94389534\n",
      "by -> chemistry:  0.7653643\n",
      "elevation -> chemistry:  0.872762\n",
      "of -> chemistry:  0.7530356\n",
      "[Ca(2+)]i -> chemistry:  0.9702405\n",
      "in -> chemistry:  0.73156077\n",
      "HAPI -> chemistry:  0.89226633\n",
      "microglial -> chemistry:  0.7177314\n",
      "cells. -> chemistry:  0.980418\n"
     ]
    }
   ],
   "source": [
    "# np.random.choice(np.arange(len(sim)), p=sim, size=int(0.15*len(sim)), replace=False)\n",
    "# output_chem.pooler_output.shape\n",
    "# output_cytokine.pooler_output.shape\n",
    "for idx, word in enumerate(sent.split(\" \")):\n",
    "    input_cytokine = tokenizer(word, return_tensors=\"pt\")\n",
    "    output_cytokine = model(**input_cytokine)\n",
    "    print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.pooler_output, output_cytokine.pooler_output).detach().numpy()[0])\n",
    "    # print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.last_hidden_state.squeeze()[1:-1], \n",
    "#                                                           output_cytokine.last_hidden_state.squeeze()[1:-1]).detach().numpy()[0])\n",
    "    \n",
    "    # print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.last_hidden_state[0][0][None], \n",
    "                                                          # output_cytokine.last_hidden_state[0][0][None]).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fbac569f-f0e1-4797-a760-94b0d6b73d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 17, 17, 18, 19, 20, 21, 21, 22, 23, 23, 24, 25, 26, 27, 28, 29,\n",
       "       29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
       "       46, 47, 47, 48, 48, 48, 49, 50, None], dtype=object)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(encoded_sent.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7173b5d4-ba28-4326-9bb0-7cfa8610b2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_chem.last_hidden_state.squeeze()[1:-1].shape\n",
    "token_ids_word = np.where(np.array(encoded_sent.word_ids()) == 2)\n",
    "word_tokens_output = output_[token_ids_word]\n",
    "word_tokens_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b623380-c754-4bfe-b461-54bd95cdae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(output_chem.last_hidden_state, output_cytokine.pooler_output).detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f67063d8-f1b9-4771-b43b-b38317107296",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'In': 0.2665925,\n",
       " 'concert': 0.2518924,\n",
       " 'with': 0.13650915,\n",
       " 'these': 0.14104173,\n",
       " 'results,': 0.16611066,\n",
       " 'we': -0.0527275,\n",
       " 'highlighted': 0.24367954,\n",
       " 'that': 0.16001697,\n",
       " 'the': 0.1787515,\n",
       " 'secretion': 0.19589587,\n",
       " 'of': 0.1480662,\n",
       " 'pro-inflammatory': 0.16328517,\n",
       " '<<': 0.10657069,\n",
       " 'cytokine': 0.11496348,\n",
       " '>>': 0.14050922,\n",
       " 'and': 0.09511185,\n",
       " 'NF-κB': 0.12212683,\n",
       " 'activation': 0.17021112,\n",
       " 'induced': 0.13242325,\n",
       " 'by': 0.14946297,\n",
       " '[[': 0.18214768,\n",
       " 'TCDD': 0.104534194,\n",
       " ']]': 0.105115205,\n",
       " 'can': 0.09944187,\n",
       " 'be': 0.15731451,\n",
       " 'mediated': 0.12850343,\n",
       " 'elevation': 0.14376499,\n",
       " '[Ca(2+)]i': 0.16997217,\n",
       " 'in': 0.1519298,\n",
       " 'HAPI': 0.12894878,\n",
       " 'microglial': 0.13394785,\n",
       " 'cells.': 0.115573525}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([output_chem.last_hidden_state[0][1].detach().numpy()], [word_tokens_output.detach().numpy()])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ea740c99-8ba6-48c8-bb82-e24e359720b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# sent = \"Hello, my dog is cute\"\n",
    "sent = \"In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.\"\n",
    "input_ids = torch.tensor(tokenizer.encode(sent)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]\n",
    "last_hidden_states = last_hidden_states[0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f9f12c8d-c494-44d3-90d8-4319b7fe0c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 768])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "62a85913-4778-43a8-a429-bccdbc0b9c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_tokens = []\n",
    "true_embs = []\n",
    "\n",
    "for idx, tok in enumerate(tokens[1:-1]):\n",
    "    text_tok = ''.join(tokenizer.decode(tok).split(' '))\n",
    "    if not text_tok.startswith('##'):\n",
    "        true_tokens.append(text_tok)\n",
    "        true_embs.append(last_hidden_states[idx])\n",
    "    else:\n",
    "        true_tokens[-1] = true_tokens[-1]+text_tok[2:]\n",
    "        true_embs[-1] = torch.add(true_embs[-1],last_hidden_states[idx])\n",
    "        \n",
    "true_embs = torch.stack(true_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "53c1e672-9988-45b8-89b7-ae58578b35e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in -> chemistry:  0.26659256\n",
      "concert -> chemistry:  0.25189242\n",
      "with -> chemistry:  0.13650917\n",
      "these -> chemistry:  0.14104171\n",
      "results -> chemistry:  0.16611068\n",
      ", -> chemistry:  -0.05272751\n",
      "we -> chemistry:  0.24367955\n",
      "highlighted -> chemistry:  0.16001694\n",
      "that -> chemistry:  0.1787515\n",
      "the -> chemistry:  0.19589588\n",
      "secretion -> chemistry:  0.1854894\n",
      "of -> chemistry:  0.16328514\n",
      "pro -> chemistry:  0.10657069\n",
      "- -> chemistry:  0.1149635\n",
      "inflammatory -> chemistry:  0.14050922\n",
      "< -> chemistry:  0.09511183\n",
      "< -> chemistry:  0.12212684\n",
      "cytokine -> chemistry:  0.17021114\n",
      "> -> chemistry:  0.13242324\n",
      "> -> chemistry:  0.12299916\n",
      "and -> chemistry:  0.18214767\n",
      "nf -> chemistry:  0.104534216\n",
      "- -> chemistry:  0.10511521\n",
      "κb -> chemistry:  0.09944189\n",
      "activation -> chemistry:  0.15731452\n",
      "induced -> chemistry:  0.12850343\n",
      "by -> chemistry:  0.14946294\n",
      "[ -> chemistry:  0.143765\n",
      "[ -> chemistry:  0.14806618\n",
      "tcdd -> chemistry:  0.1699722\n",
      "] -> chemistry:  0.1519298\n",
      "] -> chemistry:  0.1289488\n",
      "can -> chemistry:  0.13394783\n",
      "be -> chemistry:  0.11557354\n",
      "mediated -> chemistry:  0.112891145\n",
      "by -> chemistry:  0.1591808\n",
      "elevation -> chemistry:  0.12696457\n",
      "of -> chemistry:  0.12090352\n",
      "[ -> chemistry:  0.11809434\n",
      "ca -> chemistry:  0.16979319\n",
      "( -> chemistry:  0.14103599\n",
      "2 -> chemistry:  0.19188897\n",
      "+ -> chemistry:  0.1578794\n",
      ") -> chemistry:  0.16056815\n",
      "] -> chemistry:  0.1455566\n",
      "i -> chemistry:  0.20629923\n",
      "in -> chemistry:  0.13271002\n",
      "hapi -> chemistry:  0.12703519\n",
      "microglial -> chemistry:  0.10768199\n",
      "cells -> chemistry:  0.116800934\n",
      ". -> chemistry:  -0.0073787966\n"
     ]
    }
   ],
   "source": [
    "for idx, word in enumerate(true_tokens):\n",
    "    print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.last_hidden_state[0][1:-1], \n",
    "                                                          true_embs[idx][None]).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9569ba06-0a60-4b02-a7d4-58d2eacf3f71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'concert',\n",
       " 'with',\n",
       " 'these',\n",
       " 'results',\n",
       " ',',\n",
       " 'we',\n",
       " 'highlighted',\n",
       " 'that',\n",
       " 'the',\n",
       " 'secretion',\n",
       " 'of',\n",
       " 'pro',\n",
       " '-',\n",
       " 'inflammatory',\n",
       " '<',\n",
       " '<',\n",
       " 'cytokine',\n",
       " '>',\n",
       " '>',\n",
       " 'and',\n",
       " 'nf',\n",
       " '-',\n",
       " 'κb',\n",
       " 'activation',\n",
       " 'induced',\n",
       " 'by',\n",
       " '[',\n",
       " '[',\n",
       " 'tcdd',\n",
       " ']',\n",
       " ']',\n",
       " 'can',\n",
       " 'be',\n",
       " 'mediated',\n",
       " 'by',\n",
       " 'elevation',\n",
       " 'of',\n",
       " '[',\n",
       " 'ca',\n",
       " '(',\n",
       " '2',\n",
       " '+',\n",
       " ')',\n",
       " ']',\n",
       " 'i',\n",
       " 'in',\n",
       " 'hapi',\n",
       " 'microglial',\n",
       " 'cells',\n",
       " '.']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "46a1f30a-1471-4e87-8f3c-b785ffffedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokenizer_roberta_fast = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model_roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "sent = \"In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ded38107-1b0e-4ff4-93b1-32eff446bef3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "in\n",
      "concert\n",
      "with\n",
      "these\n",
      "results\n",
      ",\n",
      "we\n",
      "highlighted\n",
      "that\n",
      "the\n",
      "secret\n",
      "##ion\n",
      "of\n",
      "pro\n",
      "-\n",
      "inflammatory\n",
      "<\n",
      "<\n",
      "cy\n",
      "##tok\n",
      "##ine\n",
      ">\n",
      ">\n",
      "and\n",
      "n\n",
      "##f\n",
      "-\n",
      "κ\n",
      "##b\n",
      "activation\n",
      "induced\n",
      "by\n",
      "[\n",
      "[\n",
      "tc\n",
      "##dd\n",
      "]\n",
      "]\n",
      "can\n",
      "be\n",
      "mediated\n",
      "by\n",
      "elevation\n",
      "of\n",
      "[\n",
      "ca\n",
      "(\n",
      "2\n",
      "+\n",
      ")\n",
      "]\n",
      "i\n",
      "in\n",
      "ha\n",
      "##pi\n",
      "micro\n",
      "##glia\n",
      "##l\n",
      "cells\n",
      ".\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "tokens_bert = tokenizer_bert(sent)['input_ids']\n",
    "for tok in tokens_bert:\n",
    "    print(tokenizer_bert.decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "6204d8f4-a352-4497-827a-3f9d59a48e33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in -> chemistry:  0.73156077\n",
      "concert -> chemistry:  0.87898844\n",
      "with -> chemistry:  0.64207655\n",
      "these -> chemistry:  0.59284836\n",
      "results -> chemistry:  0.8170194\n",
      "we -> chemistry:  0.89152473\n",
      "highlighted -> chemistry:  0.9565176\n",
      "that -> chemistry:  0.96629876\n",
      "the -> chemistry:  0.60394984\n",
      "secretion -> chemistry:  0.83476865\n",
      "of -> chemistry:  0.7530356\n",
      "pro-inflammatory -> chemistry:  0.96491736\n",
      "<< -> chemistry:  0.91706\n",
      "cytokine -> chemistry:  0.9815494\n",
      ">> -> chemistry:  0.7909668\n",
      "and -> chemistry:  0.88205796\n",
      "nf-κb -> chemistry:  0.8645779\n",
      "activation -> chemistry:  0.85931265\n",
      "induced -> chemistry:  0.81914693\n",
      "by -> chemistry:  0.7653643\n",
      "[[ -> chemistry:  0.7459818\n",
      "tcdd -> chemistry:  0.96978515\n",
      "]] -> chemistry:  0.84205204\n",
      "can -> chemistry:  0.79442346\n",
      "be -> chemistry:  0.8243005\n",
      "mediated -> chemistry:  0.94389534\n",
      "by -> chemistry:  0.7653643\n",
      "elevation -> chemistry:  0.872762\n",
      "of -> chemistry:  0.7530356\n",
      "[ca(2+)]i -> chemistry:  0.9702405\n",
      "in -> chemistry:  0.73156077\n",
      "hapi -> chemistry:  0.89226633\n",
      "microglial -> chemistry:  0.7177314\n",
      "cells -> chemistry:  0.89669096\n"
     ]
    }
   ],
   "source": [
    "# input_chem = tokenizer_roberta('chemistry', return_tensors=\"pt\")\n",
    "# output_chem = model_roberta(**input_chem)\n",
    "input_chem = tokenizer_bert('chemistry', return_tensors=\"pt\")\n",
    "output_chem = model_bert(**input_chem)\n",
    "        \n",
    "sim = []\n",
    "clean_sent = sent.strip('.!').lower()\n",
    "for idx, word in enumerate(sent.lower().split(\" \")):\n",
    "    # input_cytokine = tokenizer_roberta(word, return_tensors=\"pt\")\n",
    "    # output_cytokine = model_roberta(**input_cytokine)\n",
    "    word = word.strip('.,')\n",
    "    input_cytokine = tokenizer_bert(word, return_tensors=\"pt\")\n",
    "    output_cytokine = model_bert(**input_cytokine)\n",
    "    print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.pooler_output, output_cytokine.pooler_output).detach().numpy()[0])\n",
    "    sim.append(torch.cosine_similarity(output_chem.pooler_output, output_cytokine.pooler_output).detach().numpy()[0])\n",
    "    # print(word+' -> chemistry: ', torch.cosine_similarity(output_chem.last_hidden_state.mean(dim=1), output_cytokine.last_hidden_state.mean(dim=1)).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "eb640432-8196-4e7d-a2a9-ec7a14d0b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_seed_emb():\n",
    "    input_chem = tokenizer_bert('chemistry', return_tensors=\"pt\")\n",
    "    output_chem = model_bert(**input_chem)\n",
    "    return output_chem\n",
    "\n",
    "def get_similarity(sent):\n",
    "    output_chem = get_seed_emb()\n",
    "    sim = []\n",
    "    sent = sent.strip('.!')\n",
    "    for idx, word in enumerate(sent.split(\" \")):\n",
    "        word = word.strip('.,')\n",
    "        input_cytokine = tokenizer_bert(word, return_tensors=\"pt\")\n",
    "        output_cytokine = model_bert(**input_cytokine)\n",
    "        sim.append(torch.cosine_similarity(output_chem.pooler_output, output_cytokine.pooler_output).detach().numpy()[0])\n",
    "    sim = np.array(sim)\n",
    "    sim = sim/(1.0+np.exp(-sim))\n",
    "    sim /= sim.sum()\n",
    "    K = int(0.15*len(sim))\n",
    "    indices = np.argpartition(sim,-K)[-K:]\n",
    "    # sim = (cur_sim+1)/2\n",
    "    # sim /= sim.sum()\n",
    "    # order = sim.argsort()\n",
    "    # ranks = order.argsort()+1\n",
    "    # temp = ranks/(1.0+np.exp(-ranks))\n",
    "    # temp /= temp.sum()\n",
    "    return sim, indices #np.random.choice(np.arange(len(sim)), size=int(0.15*len(sim)), replace=False, p=temp)\n",
    "\n",
    "sim, indices = get_similarity(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "31ed9778-2b71-4851-963c-77093db89dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02503623, 0.03148359, 0.02132532, 0.0193536 , 0.02872522,\n",
       "        0.03204976, 0.03502716, 0.0354812 , 0.0197938 , 0.02950831,\n",
       "        0.02595027, 0.03541699, 0.03321123, 0.03619216, 0.02758615,\n",
       "        0.03162197, 0.03083612, 0.03060045, 0.02881878, 0.026479  ,\n",
       "        0.02564906, 0.03564341, 0.02983129, 0.02773656, 0.02904577,\n",
       "        0.0344435 , 0.026479  , 0.0312034 , 0.02595027, 0.03566461,\n",
       "        0.02503623, 0.03208334, 0.02445234, 0.03228386], dtype=float32),\n",
       " array([0.03541699, 0.0354812 , 0.03564341, 0.03619216, 0.03566461],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim, sim[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "6d056c8b-f358-4243-8504-5ecda7d4ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_emb():\n",
    "    input_chem = tokenizer_bert('chemistry', return_tensors=\"pt\")\n",
    "    output_chem = model_bert(**input_chem)\n",
    "    return output_chem\n",
    "\n",
    "def get_similarity(sent):\n",
    "    output_chem = get_seed_emb()\n",
    "    clean_sent = [word.strip('.,') for word in sent.lower().strip('.!').split(\" \")]\n",
    "    input_cytokine = tokenizer_bert.batch_encode_plus(clean_sent, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_cytokine = model_bert(**input_cytokine)\n",
    "    sim = []\n",
    "    \n",
    "    for idx in range(output_cytokine.pooler_output.shape[0]):\n",
    "        sim.append(torch.cosine_similarity(output_chem.pooler_output, output_cytokine.pooler_output[idx][None]).detach().numpy()[0])\n",
    "    sim = np.array(sim)\n",
    "    sim = np.divide(sim, (1.0+np.exp(-sim)))\n",
    "    sim /= sim.sum()\n",
    "    K = int(0.25*len(sim))\n",
    "    # indices = np.argpartition(sim,-K)[-K:]\n",
    "    # indices = sorted(indices, reverse=True)\n",
    "    indices = (-sim).argsort()[:K]\n",
    "    words = sent.split(\" \")\n",
    "    sim_word = [words[idx] for idx in indices]\n",
    "    return sim, sim_word, indices#sim_word, indices\n",
    "\n",
    "def get_masked_indices(sent, tokenized_sent):\n",
    "    sim, sim_word, indices = get_similarity(sent)\n",
    "    idx = 1\n",
    "    words = [x for x in sent.split(' ')]\n",
    "    enc = tokenizer_roberta.batch_encode_plus(words, add_special_tokens=False, add_prefix_space=True)\n",
    "    desired_output = []\n",
    "    masked_tok_indices = []\n",
    "    masked_tok_dict = {}\n",
    "    total_len = 0\n",
    "    for w, token in zip(words, enc.input_ids):\n",
    "        tokenoutput = []\n",
    "        tokenoutput.extend(range(idx, idx+len(token)))\n",
    "        idx += len(token)\n",
    "        total_len += len(token)\n",
    "        if w in sim_word:\n",
    "            masked_tok_dict[w] = tokenoutput\n",
    "    for w in sim_word:\n",
    "        masked_tok_indices.extend(masked_tok_dict[w])\n",
    "    masked_tok_indices = np.array(masked_tok_indices)[np.random.choice(len(masked_tok_indices), size=int(total_len*0.15), replace=False)]#maksed_tok_indices[:int(total_len*0.15)]\n",
    "    mask = torch.full(tokenized_sent.shape, False)\n",
    "    mask[masked_tok_indices] = True\n",
    "    return mask\n",
    "    # return maksed_tok_indices#, sim, enc.input_ids, sim_word\n",
    "\n",
    "sents = ['In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.',\n",
    "         'In phosphotriesterase and physostigmine-treated mice, a 4- and 2-fold higher << sarin >> dose, respectively, was needed to cause a 50% inhibition of brain [[ AChE ]] activity.']\n",
    "# maksed_tok_indicess, sim, enc_input_ids, sim_word = get_masked_indices(sents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "113ea1d5-2258-42f4-9650-20c156339806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maksed_tok_indicess\n",
    "from torch.nn.utils.rnn import pad_sequence as torch_pad_sequence\n",
    "def pad_sequence(all_egs, pad_token_id):\n",
    "\tif pad_token_id is None:\n",
    "\t\treturn torch_pad_sequence(all_egs, batch_first=True)\n",
    "\telse:\n",
    "\t\treturn torch_pad_sequence(all_egs, batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "out = tokenizer_roberta.batch_encode_plus(sents, add_special_tokens=True, truncation=True, \n",
    "                                          max_length=500, return_special_tokens_mask=True)\n",
    "all_egs = [torch.tensor(x) for x in out[\"input_ids\"]]\n",
    "out = pad_sequence(all_egs, tokenizer_roberta.pad_token_id)\n",
    "maksed_tok_indicess = torch.stack([get_masked_indices(x, out[idx]) for idx, x in enumerate(sents)])\n",
    "\n",
    "probability_matrix = torch.full(out.shape, False)\n",
    "probability_matrix[maksed_tok_indicess] = True\n",
    "# probability_matrix, maksed_tok_indicess\n",
    "# torch.nested.nested_tensor(maksed_tok_indicess)\n",
    "# maksed_tok_indicess.\n",
    "probability_matrix == maksed_tok_indicess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "2a40eddc-5216-4b97-af7a-1389a216da4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 56])"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(maksed_tok_indicess).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "50263152-fafa-4b82-9b80-2de1c6c07893",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10000 is out of bounds for dimension 0 with size 56",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84084/2148985299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprobability_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_chem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprobability_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 10000 is out of bounds for dimension 0 with size 56"
     ]
    }
   ],
   "source": [
    "probability_matrix = torch.full(input_chem.input_ids[0].shape, False)\n",
    "probability_matrix[10000] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "e6b19223-0f83-47f0-9e4c-15776ad4eb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cytokine',\n",
       "  '[Ca(2+)]i',\n",
       "  'TCDD',\n",
       "  'that',\n",
       "  'pro-inflammatory',\n",
       "  'highlighted',\n",
       "  'mediated',\n",
       "  '<<'],\n",
       " array([0.02503624, 0.03148359, 0.02132531, 0.0193536 , 0.02872521,\n",
       "        0.03204975, 0.03502716, 0.03548121, 0.0197938 , 0.0295083 ,\n",
       "        0.02595027, 0.03541698, 0.03321123, 0.03619216, 0.02758614,\n",
       "        0.03162198, 0.03083612, 0.03060045, 0.02881879, 0.02647901,\n",
       "        0.02564906, 0.03564341, 0.0298313 , 0.02773657, 0.02904578,\n",
       "        0.0344435 , 0.02647901, 0.0312034 , 0.02595027, 0.0356646 ,\n",
       "        0.02503624, 0.03208334, 0.02445235, 0.03228386], dtype=float32))"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_word, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "c685cc62-4b39-468c-87a5-28b91bcc625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True,  True, False, False, False,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False,  True, False, False, False,\n",
       "        False,  True,  True,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False])"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sim_word, sim\n",
    "proba_mat = torch.full(input_chem.input_ids[0].shape, False)\n",
    "proba_mat[maksed_tok_indicess] = True\n",
    "proba_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "6231a336-f28e-4e12-9d8a-26e71450a3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In concert with these results, we highlighted that the secretion of<s><s>inflammatory << cytok<s> >> and NF-κB activation induced by [[ T<s>D ]] can be<s> by elevation of [<s><s><s>+)]i in HAPI microglial cells.'"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chem = tokenizer_roberta(sent, return_tensors=\"pt\")\n",
    "input_chem.input_ids[0][maksed_tok_indicess] = 0\n",
    "tokenizer_roberta.decode(input_chem['input_ids'][0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "98c29067-486c-4488-a83a-f302c94360bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5062587 , 0.50787026, 0.50533116, 0.5048383 , 0.5071808 ,\n",
       "       0.50801176, 0.5087559 , 0.5088694 , 0.5049483 , 0.50737655,\n",
       "       0.5064872 , 0.5088533 , 0.50830203, 0.5090471 , 0.5068961 ,\n",
       "       0.5079048 , 0.50770843, 0.50764954, 0.5072042 , 0.5066194 ,\n",
       "       0.5064119 , 0.50890994, 0.5074573 , 0.5069337 , 0.507261  ,\n",
       "       0.50861   , 0.5066194 , 0.5078002 , 0.5064872 , 0.5089152 ,\n",
       "       0.5062587 , 0.50802016, 0.50611275, 0.5080703 ], dtype=float32)"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent\n",
    "a = np.array([0.1,0.2,0.3])\n",
    "1.0/(1.0+np.exp(-sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "cf5c8db3-3820-417f-b2d1-32baa6349323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean_sent = [word.strip('.,') for word in sent.strip('.!').split()]\n",
    "input_chem = tokenizer_roberta(sent, return_tensors=\"pt\")\n",
    "# for tok in input_chem['input_ids'][0][1:-1]:\n",
    "#     print(tokenizer_roberta.decode(tok))\n",
    "# sent = \"Hello, my dog is cute\"\n",
    "mult_sent = [\"In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.\",\n",
    "            \"Hello, my dog is cute\"]\n",
    "sim, sim_word, indices = get_similarity(sent)\n",
    "idx = 1\n",
    "# mult_words = [[x for x in sent.split(' ')] for sent in mult_sent]\n",
    "words = [x for x in sent.split(' ')]\n",
    "enc = tokenizer_roberta.batch_encode_plus(words, add_special_tokens=False, add_prefix_space=True)\n",
    "desired_output = []\n",
    "maksed_tok_indices = []\n",
    "for w, token in zip(words, enc.input_ids):\n",
    "    tokenoutput = []\n",
    "    tokenoutput.extend(range(idx, idx+len(token)))\n",
    "    idx += len(token)\n",
    "    if w in sim_word:\n",
    "        maksed_tok_indices.extend(tokenoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "6e81ec02-c26c-4607-81de-90ee67a1052f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84084/43557868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m             \"Hello, my dog is cute\"]\n\u001b[1;32m      3\u001b[0m \u001b[0mmult_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmult_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmult_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m         )\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "mult_sent = [\"In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.\",\n",
    "            \"Hello, my dog is cute\"]\n",
    "mult_words = [[x for x in sent.split(' ')] for sent in mult_sent]\n",
    "enc = tokenizer_roberta.batch_encode_plus(mult_words, add_special_tokens=False, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "8566ddae-67e2-4cbc-816f-b41635da4159",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'concert',\n",
       "  'with',\n",
       "  'these',\n",
       "  'results,',\n",
       "  'we',\n",
       "  'highlighted',\n",
       "  'that',\n",
       "  'the',\n",
       "  'secretion',\n",
       "  'of',\n",
       "  'pro-inflammatory',\n",
       "  '<<',\n",
       "  'cytokine',\n",
       "  '>>',\n",
       "  'and',\n",
       "  'NF-κB',\n",
       "  'activation',\n",
       "  'induced',\n",
       "  'by',\n",
       "  '[[',\n",
       "  'TCDD',\n",
       "  ']]',\n",
       "  'can',\n",
       "  'be',\n",
       "  'mediated',\n",
       "  'by',\n",
       "  'elevation',\n",
       "  'of',\n",
       "  '[Ca(2+)]i',\n",
       "  'in',\n",
       "  'HAPI',\n",
       "  'microglial',\n",
       "  'cells.'],\n",
       " ['Hello,', 'my', 'dog', 'is', 'cute']]"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "c7c2b382-ddb4-4b1f-912a-debc0d40a5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 9,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 36,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 48,\n",
       " 49,\n",
       " 53,\n",
       " 54]"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maksed_tok_indices\n",
    "# range(idx, idx+len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "feb64523-0a4b-4683-bf74-9dacf1503125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 49, 53, 54, 29, 30, 31, 36]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maksed_tok_indicess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "ba9ad6b9-d42a-4cd9-a5d7-95c9ea306c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_roberta.decode(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "84989810-4bee-4b95-b370-9594825f88d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In concert with these results, we<s><s> the secretion of<s><s><s><s><s><s> >> and NF-κB activation induced by [[<s><s><s> ]] can be<s> by elevation of<s><s><s><s><s><s><s> in<s><s> microglial<s><s>'"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chem = tokenizer_roberta(sent, return_tensors=\"pt\")\n",
    "input_chem.input_ids[0][maksed_tok_indices] = 0\n",
    "tokenizer_roberta.decode(input_chem['input_ids'][0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "103be959-ced8-4d73-85e3-81518a56c558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 18, 40, 41, 42, 43, 44, 45]"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maksed_tok_indicess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "47f75789-c2a7-4f95-9d12-b8b44fed0aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In concert with these results, we highlighted<s> the secretion of pro<s>inflammatory<s><s><s> >> and NF-κB activation induced by [[ T<s><s> ]] can be mediated by elevation of [Ca(2<s>)]i in HAPI microglial cells.'"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "27f231a7-3620-4fc0-b3d9-9fc7bc7aca34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In concert with these results, we highlighted that the secretion of pro-inflammatory << cytokine >> and NF-κB activation induced by [[ TCDD ]] can be mediated by elevation of [Ca(2+)]i in HAPI microglial cells.',\n",
       " array([0.02503624, 0.03148359, 0.02132531, 0.0193536 , 0.02872521,\n",
       "        0.03204975, 0.03502716, 0.03548121, 0.0197938 , 0.0295083 ,\n",
       "        0.02595027, 0.03541698, 0.03321123, 0.03619216, 0.02758614,\n",
       "        0.03162198, 0.03083612, 0.03060045, 0.02881879, 0.02647901,\n",
       "        0.02564906, 0.03564341, 0.0298313 , 0.02773657, 0.02904578,\n",
       "        0.0344435 , 0.02647901, 0.0312034 , 0.02595027, 0.0356646 ,\n",
       "        0.02503624, 0.03208334, 0.02445235, 0.03228386], dtype=float32))"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "9bc62a9f-3f35-4396-945e-3030638450b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', [1]), ('concert', [2]), ('with', [3]), ('these', [4]), ('results,', [5, 6]), ('we', [7]), ('highlighted', [8]), ('that', [9]), ('the', [10]), ('secretion', [11]), ('of', [12]), ('pro-inflammatory', [13, 14, 15]), ('<<', [16]), ('cytokine', [17, 18]), ('>>', [19]), ('and', [20]), ('NF-κB', [21, 22, 23, 24]), ('activation', [25]), ('induced', [26]), ('by', [27]), ('[[', [28]), ('TCDD', [29, 30, 31]), (']]', [32, 33]), ('can', [34]), ('be', [35]), ('mediated', [36]), ('by', [37]), ('elevation', [38]), ('of', [39]), ('[Ca(2+)]i', [40, 41, 42, 43, 44, 45, 46]), ('in', [47]), ('HAPI', [48, 49]), ('microglial', [50, 51, 52]), ('cells.', [53, 54])] 34\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_roberta.convert_ids_to_tokens(input_chem['input_ids'][0][1:-1])\n",
    "idx = 1\n",
    "\n",
    "enc =[(x, tokenizer_roberta.encode(x, add_special_tokens=False, add_prefix_space=True)) for x in sent.split()]\n",
    "\n",
    "desired_output = []\n",
    "\n",
    "for w, token in enc:\n",
    "    tokenoutput = []\n",
    "    for ids in token:\n",
    "        tokenoutput.append(idx)\n",
    "        idx +=1\n",
    "    desired_output.append((w, tokenoutput))\n",
    "\n",
    "print(desired_output, len(desired_output))\n",
    "\n",
    "# desired_output = []\n",
    "# encoded = tokenizer_roberta_fast(sent)\n",
    "# for word_id in encoded.word_ids():\n",
    "#     if word_id is not None:\n",
    "#         start, end = encoded.word_to_tokens(word_id)\n",
    "#         if start == end - 1:\n",
    "#             tokens = [start]\n",
    "#         else:\n",
    "#             tokens = [start, end-1]\n",
    "#         if len(desired_output) == 0 or desired_output[-1] != tokens:\n",
    "#             desired_output.append(tokens)\n",
    "            \n",
    "# print(desired_output, len(desired_output))\n",
    "# tokenizer_roberta.convert_tokens_to_ids(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "8a603005-d04e-4e25-b855-daf00abba156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  1121,  4192,    19,   209,   775,     6,    52,  6263,     0,\n",
       "             5, 46886,     9,     0,     0,     0, 48188,     0,     0,  8488,\n",
       "             8, 33861,    12, 48103,   387, 29997, 26914,    30, 48395,     0,\n",
       "             0,     0, 27779,   742,    64,    28, 43219,    30, 25361,     9,\n",
       "             0,     0,     0,     0,     0,     0,     0,    11,   289, 40104,\n",
       "          5177,  7210,  2617,  4590,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "1c9871a6-f902-42e9-bac2-505a0e96c8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 22,  3,  1, 14, 24, 29, 31,  2, 17,  9, 30, 27, 34, 12, 23, 20,\n",
       "       19, 15, 11,  7, 32, 18, 13, 16, 28, 10, 21,  8, 33,  6, 25,  4, 26])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = sim.argsort()\n",
    "ranks = order.argsort()+1\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "53147f39-8bfa-4ec3-97c8-7d77e6fc7589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In ->  0.025036242\n",
      "concert ->  0.031483594\n",
      "with ->  0.021325313\n",
      "these ->  0.019353596\n",
      "results, ->  0.028725207\n",
      "we ->  0.03204975\n",
      "highlighted ->  0.03502716\n",
      "that ->  0.03548121\n",
      "the ->  0.019793795\n",
      "secretion ->  0.029508302\n",
      "of ->  0.025950268\n",
      "pro-inflammatory ->  0.03541698\n",
      "<< ->  0.033211228\n",
      "cytokine ->  0.036192164\n",
      ">> ->  0.027586136\n",
      "and ->  0.03162198\n",
      "NF-κB ->  0.030836122\n",
      "activation ->  0.030600447\n",
      "induced ->  0.028818786\n",
      "by ->  0.026479013\n",
      "[[ ->  0.025649063\n",
      "TCDD ->  0.035643414\n",
      "]] ->  0.029831296\n",
      "can ->  0.027736565\n",
      "be ->  0.029045783\n",
      "mediated ->  0.034443498\n",
      "by ->  0.026479013\n",
      "elevation ->  0.031203397\n",
      "of ->  0.025950268\n",
      "[Ca(2+)]i ->  0.035664603\n",
      "in ->  0.025036242\n",
      "HAPI ->  0.032083336\n",
      "microglial ->  0.024452353\n",
      "cells. ->  0.03228386\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "438af4e5-0505-40da-abbf-01cf181abb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 10])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cytokine.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "195bc762-835c-4dc1-b7a4-81dad447121f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cytokine.pooler_output[0][None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "92450e96-840f-4919-aaa2-c057dea59b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 22,  3,  1, 14, 24, 29, 31,  2, 17,  9, 30, 27, 34, 12, 23, 20,\n",
       "       19, 15, 11,  7, 32, 18, 13, 16, 28, 10, 21,  8, 33,  6, 25,  4, 26])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = sim.argsort()\n",
    "ranks = order.argsort()+1\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "bb08ad2d-a65b-4ac8-aa52-89b470194df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02782355, 0.03019249, 0.02638568, 0.02559466, 0.02919674,\n",
       "       0.03039393, 0.03143827, 0.03159544, 0.02577304, 0.02948195,\n",
       "       0.02816862, 0.03157324, 0.03080424, 0.03184049, 0.02877812,\n",
       "       0.03024181, 0.02996094, 0.02987633, 0.02923093, 0.02836672,\n",
       "       0.02805528, 0.03165146, 0.02959898, 0.02883366, 0.02931374,\n",
       "       0.03123545, 0.02836672, 0.03009244, 0.02816862, 0.03165877,\n",
       "       0.02782355, 0.03040585, 0.02760134, 0.03047694], dtype=float32)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "ae3b5d58-3e47-4911-b1d3-9c097add8707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00835809, 0.03702337, 0.00480921, 0.00123028, 0.02356031,\n",
       "       0.04038914, 0.04880354, 0.0521693 , 0.00296455, 0.02860897,\n",
       "       0.01514406, 0.05048642, 0.04543778, 0.05721794, 0.02019444,\n",
       "       0.03870626, 0.03365761, 0.03197473, 0.0252432 , 0.01851138,\n",
       "       0.01176943, 0.05385218, 0.03029185, 0.0218774 , 0.02692609,\n",
       "       0.04712066, 0.01682804, 0.03534049, 0.01345853, 0.05553506,\n",
       "       0.01007232, 0.04207202, 0.00661045, 0.0437549 ])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ranks/(1.0+np.exp(-ranks))\n",
    "temp /= temp.sum()\n",
    "# order_temp = temp.argsort()\n",
    "# ranks_temp = order_temp.argsort()+1\n",
    "# ranks_temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f3b45353-e312-4831-ac09-edd03c112a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks_temp == ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "ef00edf7-5243-4107-9aad-72eb3b64e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7071557 , 0.7083574 , 0.706427  , 0.7060264 , 0.70785207,\n",
       "       0.70845956, 0.7089898 , 0.7090696 , 0.70611674, 0.7079969 ,\n",
       "       0.7073307 , 0.70905834, 0.70866793, 0.7091942 , 0.7076398 ,\n",
       "       0.7083823 , 0.7082399 , 0.7081969 , 0.70786947, 0.7074311 ,\n",
       "       0.7072731 , 0.7090981 , 0.7080562 , 0.707668  , 0.7079114 ,\n",
       "       0.7088868 , 0.7074311 , 0.70830655, 0.7073307 , 0.70910174,\n",
       "       0.7071557 , 0.70846564, 0.7070431 , 0.7085017 ], dtype=float32)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1+np.exp(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c5aa3740-6ed7-407c-bc9e-387a556ac1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),) tensor([], size=(0, 768), grad_fn=<IndexBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_84084/2040357402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# word_tokens_output = word_tokens_output.mean(dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokens_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokens_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "input_chem = tokenizer_roberta_fast('chemistry', return_tensors=\"pt\")\n",
    "output_chem = model_roberta(**input_chem)\n",
    "\n",
    "encoded_sent = tokenizer_roberta_fast(sent, return_tensors=\"pt\")\n",
    "output_sent = model_roberta(**encoded_sent)\n",
    "output_ = output_sent.last_hidden_state.squeeze()\n",
    "\n",
    "sim = []\n",
    "word_sim = {}\n",
    "for idx, word in enumerate(encoded_sent['input_ids'][0][1:-1]):#(sent.split(' ')):\n",
    "    token_ids_word = np.where(np.array(encoded_sent.word_ids()) == idx)\n",
    "    word_tokens_output = output_[token_ids_word]\n",
    "    # word_tokens_output = word_tokens_output.mean(dim=0)\n",
    "    try:\n",
    "        sim.append(torch.cosine_similarity(output_chem.last_hidden_state.mean(dim=1), word_tokens_output).detach().numpy()[0])\n",
    "    except:\n",
    "        print(token_ids_word, word_tokens_output)\n",
    "        raise\n",
    "    # sim.append(cosine_similarity([output_chem.last_hidden_state[0][1].detach().numpy()], [word_tokens_output.detach().numpy()])[0][0])\n",
    "    word_sim[word] = sim[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a1d0c6d8-2905-4d91-9c0c-0b93870b0794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1121,  4192,    19,   209,   775,     6,    52,  6263,    14,\n",
       "             5, 46886,     9,  1759,    12, 33824, 48188, 45837,   833,  8488,\n",
       "             8, 33861,    12, 48103,   387, 29997, 26914,    30, 48395,   255,\n",
       "         11579,   495, 27779,   742,    64,    28, 43219,    30, 25361,     9,\n",
       "           646, 38593,  1640,   176,  2744, 46077,   118,    11,   289, 40104,\n",
       "          5177,  7210,  2617,  4590,     4,     2]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sent['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b0ef7fa4-1f3d-402d-b0d2-194654c2a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_chem.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932126f-1c53-4c0c-ac0b-2af2fe6d8a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ae4850-183f-4157-93f7-13edbd9c067f",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8bda57-35bb-494d-8559-e78ce5bdb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertTokenizerFast, RobertaModel, RobertaTokenizerFast\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer_bert = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_roberta_fast = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "model_roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def get_seed_emb():\n",
    "    input_chem = self.dataOpts.tokenizer_bert('chemistry', return_tensors=\"pt\")\n",
    "    # input_chem = tokenizer_bert.batch_encode_plus(['language', 'text', 'model', 'information', 'grammar', 'lexical'], add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_chem = model_bert(**input_chem)\n",
    "    return output_chem\n",
    "\n",
    "emb_chem = get_seed_emb()\n",
    "\n",
    "def get_similarity(sents):\n",
    "    clean_sent = [word.strip('.,') for sent in sents for word in sent.lower().strip('.!').split(\" \")]\n",
    "    input_cytokine = tokenizer_bert.batch_encode_plus(clean_sent, add_special_tokens=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output_cytokine = model_bert(**input_cytokine)\n",
    "    sim = []\n",
    "    for idx in tqdm.tqdm(range(output_cytokine.pooler_output.shape[0])):\n",
    "        if emb_chem.pooler_output.shape[0] > 1:\n",
    "            temp_sim = -1000.00\n",
    "            for sidx in range(emb_chem.pooler_output.shape[0]):\n",
    "                tsim = torch.cosine_similarity(emb_chem.pooler_output[sidx][None], output_cytokine.pooler_output[idx][None]).detach().numpy()[0]\n",
    "                if temp_sim < tsim:\n",
    "                    temp_sim = tsim\n",
    "            sim.append(temp_sim)\n",
    "        else:\n",
    "            sim.append(torch.cosine_similarity(emb_chem.pooler_output, output_cytokine.pooler_output[idx][None]).detach().numpy()[0])\n",
    "    sim = np.array(sim)\n",
    "    sim = np.divide(sim, (1.0+np.exp(-sim)))\n",
    "    sim /= sim.sum()\n",
    "    K = int(0.50*len(sim)) #int(0.15*len(sim))\n",
    "    #indices = np.argpartition(sim,-K)[-K:]\n",
    "    indices = (-sim).argsort()[:K]\n",
    "    words = []\n",
    "    for sent in sents:\n",
    "        words.extend(sent.split(\" \"))\n",
    "    sim_word = [words[idx] for idx in indices]\n",
    "    return sim_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766fb465-ef14-4c4f-8294-bbdc0fc7e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "# docs = json.load(open('/work/sakter/AANG/datasets/chemprot/train.jsonl', 'r'))\n",
    "\n",
    "with open('/work/sakter/AANG/datasets/chemprot/train.txt', encoding=\"utf-8\") as f:\n",
    "    token_counter = Counter()\n",
    "    lines = []\n",
    "    all_tokens = []\n",
    "    capitalizations = []\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) < 2: # Remove all single letter or empty lines\n",
    "            continue\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda689f-72fc-4548-b05d-c045cfd9532b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4169 [00:00<?, ?it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2433.38it/s]\u001b[A\n",
      "  0%|          | 1/4169 [00:01<1:14:08,  1.07s/it]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2662.74it/s]\u001b[A\n",
      "  0%|          | 2/4169 [00:01<1:06:27,  1.04it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2629.82it/s]\u001b[A\n",
      "  0%|          | 3/4169 [00:02<1:05:16,  1.06it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2683.19it/s]\u001b[A\n",
      "  0%|          | 4/4169 [00:03<1:03:57,  1.09it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2648.93it/s]\u001b[A\n",
      "  0%|          | 5/4169 [00:04<1:02:57,  1.10it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2630.34it/s]\u001b[A\n",
      "  0%|          | 6/4169 [00:05<1:02:23,  1.11it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2636.38it/s]\u001b[A\n",
      "  0%|          | 7/4169 [00:06<1:02:01,  1.12it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2611.22it/s]\u001b[A\n",
      "  0%|          | 8/4169 [00:07<1:02:02,  1.12it/s]\n",
      "  0%|          | 0/277 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 277/277 [00:00<00:00, 2382.72it/s]\u001b[A\n",
      "  0%|          | 9/4169 [00:08<1:02:06,  1.12it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2599.09it/s]\u001b[A\n",
      "  0%|          | 10/4169 [00:09<1:03:24,  1.09it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2607.59it/s]\u001b[A\n",
      "  0%|          | 11/4169 [00:10<1:04:14,  1.08it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2612.06it/s]\u001b[A\n",
      "  0%|          | 12/4169 [00:11<1:04:42,  1.07it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2625.30it/s]\u001b[A\n",
      "  0%|          | 13/4169 [00:12<1:04:58,  1.07it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2658.92it/s]\u001b[A\n",
      "  0%|          | 14/4169 [00:12<1:05:05,  1.06it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2657.01it/s]\u001b[A\n",
      "  0%|          | 15/4169 [00:13<1:05:06,  1.06it/s]\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 297/297 [00:00<00:00, 2655.19it/s]\u001b[A\n",
      "  0%|          | 16/4169 [00:14<1:05:07,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "all_sim_word = []\n",
    "import tqdm\n",
    "\n",
    "n_lines = 0\n",
    "for sent in tqdm.tqdm(lines):\n",
    "    n_lines += 1\n",
    "    sim_word = get_similarity(sent)\n",
    "    all_sim_word.extend(sim_word[:3])\n",
    "    if n_lines == 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c921f8ca-071d-4de1-aeb0-4b0756ca12ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4169"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e01618-118a-443b-a771-19456c89ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get_similarity(lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c98f18f-7350-4cfb-806e-e92f0d84dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1758/1758 [00:00<00:00, 2848.99it/s]\n",
      "100%|██████████| 1665/1665 [00:00<00:00, 2851.01it/s]\n",
      "100%|██████████| 1442/1442 [00:00<00:00, 2842.71it/s]\n",
      "100%|██████████| 1537/1537 [00:00<00:00, 2823.54it/s]\n",
      "100%|██████████| 1382/1382 [00:00<00:00, 2771.85it/s]\n",
      "100%|██████████| 1551/1551 [00:00<00:00, 2831.00it/s]\n",
      "100%|██████████| 1487/1487 [00:00<00:00, 2806.82it/s]\n",
      "100%|██████████| 1409/1409 [00:00<00:00, 2818.43it/s]\n",
      "100%|██████████| 1360/1360 [00:00<00:00, 2813.92it/s]\n",
      "100%|██████████| 1631/1631 [00:00<00:00, 2811.17it/s]\n",
      "100%|██████████| 1781/1781 [00:00<00:00, 2839.41it/s]\n",
      "100%|██████████| 1302/1302 [00:00<00:00, 2826.29it/s]\n",
      "100%|██████████| 1772/1772 [00:00<00:00, 2835.64it/s]\n",
      "100%|██████████| 1551/1551 [00:00<00:00, 2826.62it/s]\n",
      "100%|██████████| 1358/1358 [00:00<00:00, 2834.16it/s]\n",
      "100%|██████████| 1827/1827 [00:00<00:00, 2823.22it/s]\n",
      "100%|██████████| 1593/1593 [00:00<00:00, 2827.39it/s]\n",
      "100%|██████████| 1688/1688 [00:00<00:00, 2828.53it/s]\n",
      "100%|██████████| 1288/1288 [00:00<00:00, 2826.06it/s]\n",
      "100%|██████████| 1440/1440 [00:00<00:00, 2822.77it/s]\n",
      "100%|██████████| 1385/1385 [00:00<00:00, 2820.59it/s]\n",
      "100%|██████████| 1562/1562 [00:00<00:00, 2823.25it/s]\n",
      "100%|██████████| 1437/1437 [00:00<00:00, 2819.39it/s]\n",
      "100%|██████████| 1419/1419 [00:00<00:00, 2822.93it/s]\n",
      "100%|██████████| 1220/1220 [00:00<00:00, 2821.45it/s]\n",
      "100%|██████████| 1408/1408 [00:00<00:00, 2842.34it/s]\n",
      "100%|██████████| 1483/1483 [00:00<00:00, 2831.62it/s]\n",
      "100%|██████████| 1637/1637 [00:00<00:00, 2844.10it/s]\n",
      "100%|██████████| 1841/1841 [00:00<00:00, 2840.00it/s]\n",
      "100%|██████████| 1868/1868 [00:00<00:00, 2824.06it/s]\n",
      "100%|██████████| 1543/1543 [00:00<00:00, 2846.91it/s]\n",
      "100%|██████████| 1565/1565 [00:00<00:00, 2783.27it/s]\n",
      "100%|██████████| 1532/1532 [00:00<00:00, 2815.13it/s]\n",
      "100%|██████████| 1499/1499 [00:00<00:00, 2806.92it/s]\n",
      "100%|██████████| 1521/1521 [00:00<00:00, 2823.79it/s]\n",
      "100%|██████████| 1474/1474 [00:00<00:00, 2823.40it/s]\n",
      "100%|██████████| 1776/1776 [00:00<00:00, 2830.41it/s]\n",
      "100%|██████████| 1588/1588 [00:00<00:00, 2809.98it/s]\n",
      "100%|██████████| 1316/1316 [00:00<00:00, 2831.86it/s]\n",
      "100%|██████████| 1360/1360 [00:00<00:00, 2680.45it/s]\n",
      "100%|██████████| 1713/1713 [00:00<00:00, 2824.16it/s]\n",
      "100%|██████████| 1765/1765 [00:00<00:00, 2828.80it/s]\n",
      "100%|██████████| 1445/1445 [00:00<00:00, 2832.13it/s]\n",
      "100%|██████████| 1446/1446 [00:00<00:00, 2830.70it/s]\n",
      "100%|██████████| 1466/1466 [00:00<00:00, 2821.86it/s]\n",
      "100%|██████████| 1960/1960 [00:00<00:00, 2800.19it/s]\n",
      "100%|██████████| 1479/1479 [00:00<00:00, 2831.94it/s]\n",
      "100%|██████████| 1461/1461 [00:00<00:00, 2829.38it/s]\n",
      "100%|██████████| 1638/1638 [00:00<00:00, 2835.50it/s]\n",
      "100%|██████████| 1883/1883 [00:00<00:00, 2845.25it/s]\n",
      "100%|██████████| 1899/1899 [00:00<00:00, 2812.25it/s]\n",
      "100%|██████████| 1570/1570 [00:00<00:00, 2830.38it/s]\n",
      "100%|██████████| 1678/1678 [00:00<00:00, 2859.93it/s]\n",
      "100%|██████████| 1504/1504 [00:00<00:00, 2832.58it/s]\n",
      "100%|██████████| 1668/1668 [00:00<00:00, 2851.94it/s]\n",
      "100%|██████████| 1459/1459 [00:00<00:00, 2820.62it/s]\n",
      "100%|██████████| 1533/1533 [00:00<00:00, 2832.21it/s]\n",
      "100%|██████████| 1406/1406 [00:00<00:00, 2835.92it/s]\n",
      "100%|██████████| 1317/1317 [00:00<00:00, 2855.10it/s]\n",
      "100%|██████████| 1531/1531 [00:00<00:00, 2825.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75579/402388138.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msublist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# do something with the sublist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mall_sim_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_75579/3546005954.py\u001b[0m in \u001b[0;36mget_similarity\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mclean_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.,'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msents\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minput_cytokine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0moutput_cytokine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_cytokine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_cytokine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         )\n\u001b[1;32m    538\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/sakter/anaconda3/envs/aang/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_sim_word = []\n",
    "for i in range(0, len(lines), 50):\n",
    "    # get the sublist of the next `n` elements\n",
    "    sublist = lines[i:i+50]\n",
    "    # do something with the sublist\n",
    "    all_sim_word.extend(get_similarity(sublist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda4cc0-2a66-405c-bbcf-b28ed58b09ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aang",
   "language": "python",
   "name": "aang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
